{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb94ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO - Restoring model from checkpoint...\n",
      "Agent successfully restored from dictionary checkpoint!\n",
      "Starting tournament: DQN vs 1 Random Bots\n",
      "Playing 10000 hands...\n",
      "\n",
      "--------------------------------\n",
      "Final Results (10000 hands)\n",
      "--------------------------------\n",
      "DQN Agent Payoff (Avg): 1.6314 big blinds/hand\n",
      "Random Bot 0 Payoff: 1.6314\n",
      "--------------------------------\n",
      "✅ SUCCESS: The DQN is beating the random players!\n"
     ]
    }
   ],
   "source": [
    "import rlcard\n",
    "import torch\n",
    "import os\n",
    "from rlcard.agents import RandomAgent, DQNAgent\n",
    "from rlcard.utils import tournament, Logger\n",
    "\n",
    "# 1. Configuration\n",
    "ENV_NAME = 'limit-holdem' # Make sure this matches your training env!\n",
    "NUM_PLAYERS = 2\n",
    "NUM_HANDS = 10000         # How many hands to play for the test\n",
    "MODEL_PATH = 'checkpoints/dqn_2players.pt'\n",
    "\n",
    "# 1. Setup the environment to get the correct dimensions\n",
    "env = rlcard.make('limit-holdem', config={'num_players': NUM_PLAYERS})\n",
    "\n",
    "# 2. Read the .pt file as a dictionary\n",
    "# map_location='cpu' ensures it loads safely even if trained on 'mps' or 'cuda'\n",
    "checkpoint_dict = torch.load('checkpoints/dqn_2players.pt', map_location='cpu', weights_only=False)\n",
    "\n",
    "# 3. Initialize the agent with the same architecture it was trained on\n",
    "dqn_agent = DQNAgent(\n",
    "    num_actions=env.num_actions,\n",
    "    state_shape=env.state_shape[0],\n",
    "    mlp_layers=[512, 256, 128], # This MUST match the saved model\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# 4. Restore the weights and states from the dictionary\n",
    "dqn_agent.from_checkpoint(checkpoint_dict)\n",
    "\n",
    "print(\"Agent successfully restored from dictionary checkpoint!\")\n",
    "\n",
    "# 4. Create Random Opponents\n",
    "# We need 3 random agents to fill the table\n",
    "random_agents = [RandomAgent(num_actions=env.num_actions) for _ in range(NUM_PLAYERS - 1)]\n",
    "\n",
    "# 5. Bind Agents to Environment\n",
    "# DQN is Player 0, Randoms are Players 1, 2, 3\n",
    "agents = [dqn_agent] + random_agents\n",
    "env.set_agents(agents)\n",
    "\n",
    "print(f\"Starting tournament: DQN vs {NUM_PLAYERS-1} Random Bots\")\n",
    "print(f\"Playing {NUM_HANDS} hands...\")\n",
    "\n",
    "# 6. Run Tournament\n",
    "# tournament() returns a list of average payoffs for each agent\n",
    "payoffs = tournament(env, NUM_HANDS)[0]\n",
    "\n",
    "# 7. Print Results\n",
    "dqn_payoff = payoffs\n",
    "print(\"\\n--------------------------------\")\n",
    "print(f\"Final Results ({NUM_HANDS} hands)\")\n",
    "print(\"--------------------------------\")\n",
    "print(f\"DQN Agent Payoff (Avg): {dqn_payoff:.4f} big blinds/hand\")\n",
    "\n",
    "if NUM_PLAYERS > 2:\n",
    "    for i in range(NUM_PLAYERS):\n",
    "        print(f\"Random Bot {i} Payoff: {payoffs[i]:.4f}\")\n",
    "else:\n",
    "    print(f\"Random Bot Payoff: {payoffs:.4f}\")\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "if dqn_payoff > 0:\n",
    "    print(\"✅ SUCCESS: The DQN is beating the random players!\")\n",
    "else:\n",
    "    print(\"❌ WARNING: The DQN is losing to random players.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
